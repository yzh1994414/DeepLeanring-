{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import keras \n",
    "import jieba\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout\n",
    "from keras.layers import Dense,Dropout,Activation,Input,Convolution1D,Conv1D,GlobalMaxPooling1D,MaxPooling1D,Flatten,concatenate,Embedding,GRU,Lambda, LSTM, TimeDistributed\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Embedding, LSTM,Flatten\n",
    "from gensim.models import word2vec\n",
    "import time\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "np.set_printoptions(threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.713 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "def find_str(x):\n",
    "    x1 = x.split('->')[:4]\n",
    "    return '->'.join(x1)\n",
    "\n",
    "rawdata_path = './raw_data/4月份数据.csv'\n",
    "stopkeyname = './raw_data/中文停用词表.txt'\n",
    "user_dict = './raw_data/userdict.txt'\n",
    "#读取文件\n",
    "df = pd.read_csv(rawdata_path)\n",
    "jieba.load_userdict(user_dict)\n",
    "stopkey ={}.fromkeys([line.rstrip() for line in open(stopkeyname,encoding='utf-8')])  \n",
    "#生成新的业务类别\n",
    "df['business_type_new'] = df['business_type'].apply(lambda x : find_str(x))\n",
    "type_count = pd.DataFrame({'COUNT':df.groupby(['business_type_new']).size()}).reset_index()\n",
    "filter_less = type_count[type_count.COUNT>50].business_type_new.tolist()\n",
    "df = df[df.business_type_new.isin(filter_less)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntf_train = TfidfVectorizer()\\nf1 = open('./raw_data/vocabulary_op.json','wb')\\nfile = tf_train.vocabulary_\\npickle.dump(file,f1)\\nf1.close()\\nidfs = tf_train.idf_\\nnp.save('./raw_data/idfs_op.npy',idfs)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def isNum(value):\n",
    "    try:\n",
    "        value + 1\n",
    "    except TypeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "\n",
    "def isNum(value):\n",
    "    try:\n",
    "        value + 1\n",
    "    except TypeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def cleaner(word):\n",
    "    word = re.sub(r'\\#\\.', '', word)\n",
    "    word = re.sub(r'\\n', '', word)\n",
    "    word = re.sub(r',', '', word)\n",
    "    word = re.sub(r'\\-', ' ', word)\n",
    "    word = re.sub(r'\\.', '', word)\n",
    "    word = re.sub(r'\\\\', ' ', word)\n",
    "    word = re.sub(r'\\\\x\\.+', '', word)\n",
    "    word = re.sub(r'\\d', '', word)\n",
    "    word = re.sub(r'^_.', '', word)\n",
    "    word = re.sub(r'_', ' ', word)\n",
    "    word = re.sub(r'^ ', '', word)\n",
    "    word = re.sub(r' $', '', word)\n",
    "    word = re.sub(r'\\?', '', word)\n",
    "    word = re.sub(r'|', '', word)\n",
    "    word = re.sub(r'（', '', word)\n",
    "    word = re.sub(r'）', '', word)\n",
    "    word = re.sub(r':', '', word)\n",
    "    word = re.sub(r'null', '', word)\n",
    "    word = re.sub(r'Null', '', word)\n",
    "    word = re.sub(r'[\\[,\\] ,:,),|,(,。,，,/, ：,【,】,;,！,、,？,]', '', word)\n",
    "    return word\n",
    "\n",
    "x_list_1 = []\n",
    "for i in range(len(df['CONTENT_'])):\n",
    "    word_1=[]\n",
    "    result_1 = jieba.cut(str(df['CONTENT_'].iloc[i]))\n",
    "    for segs in result_1:\n",
    "        segs = cleaner(segs)\n",
    "        if segs != ''and isNum(segs) != True:\n",
    "            word_1.append(segs)\n",
    "    x_list_1.append(' '.join(word_1))\n",
    "df['content_new'] = x_list_1\n",
    "#进行tfidf的维表数据\n",
    "\n",
    "#生成维表数据\n",
    "'''\n",
    "tf_train = TfidfVectorizer()\n",
    "f1 = open('./raw_data/vocabulary_op.json','wb')\n",
    "file = tf_train.vocabulary_\n",
    "pickle.dump(file,f1)\n",
    "f1.close()\n",
    "idfs = tf_train.idf_\n",
    "np.save('./raw_data/idfs_op.npy',idfs)\n",
    "'''\n",
    "\n",
    "# for label in df['business_type_new'].unique():\n",
    "#     while df.loc[df['business_type_new']==label].shape[0]<100:\n",
    "#         df = df.append(df.loc[df['business_type_new']==label],ignore_index=True)\n",
    "#训练tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programtools\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras_preprocessing\\text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_word_len = 4000\n",
    "tokenizer = Tokenizer(\n",
    "    nb_words=max_word_len, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   split=' ')\n",
    "tokenizer.fit_on_texts(df['content_new'])\n",
    "\n",
    "X=tokenizer.texts_to_sequences(df['content_new'])\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34559, 213)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split( X,df['business_type_new'],test_size = 0.1, random_state=2019) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder().fit(train_y)\n",
    "train_y = le.transform(train_y)\n",
    "test_y = le.transform(test_y)\n",
    "train_y_cat = to_categorical(train_y,num_classes=39)\n",
    "test_y_cat = to_categorical(test_y,num_classes=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_train():\n",
    "\n",
    "\tsentences = word2vec.Text8Corpus(\"cuttext.txt\")\n",
    "\tmodel = word2vec.Word2Vec(sentences)\n",
    "\tmodel.save('./word2vec.m')\n",
    "#\tprint(model.similarity('被害人','发生'))\n",
    "\t# print(model.most_similar(\"被告人\"))\n",
    "\tprint('finished and saved!')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_train():\n",
    "\n",
    "\tsentences = word2vec.Text8Corpus(\"cuttext.txt\")\n",
    "\tmodel = word2vec.Word2Vec(sentences)\n",
    "\tmodel.save('./word2vec.m')\n",
    "\n",
    "\tprint('finished and saved!')\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc4 in position 0: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-9863a4c899ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuttext.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content_new'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"%s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mw2v_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-1e1bd7edfead>\u001b[0m in \u001b[0;36mword2vec_train\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText8Corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuttext.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./word2vec.m'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#       print(model.similarity('被害人','发生'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programtools\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[1;32md:\\programtools\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m             self.train(\n\u001b[0;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programtools\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    934\u001b[0m         \"\"\"\n\u001b[0;32m    935\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[1;32m--> 936\u001b[1;33m             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[0;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programtools\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, sentences, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[0;32m   1589\u001b[0m             \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1591\u001b[1;33m         \u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m         logger.info(\n",
      "\u001b[1;32md:\\programtools\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[1;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1558\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1560\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1562\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programtools\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1388\u001b[0m                 \u001b[0mlast_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb' '\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# last token may have been split in two... keep for next iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1389\u001b[0m                 words, rest = (utils.to_unicode(text[:last_token]).split(),\n\u001b[1;32m-> 1390\u001b[1;33m                                text[last_token:].strip()) if last_token >= 0 else ([], text)\n\u001b[0m\u001b[0;32m   1391\u001b[0m                 \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_sentence_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programtools\\anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[1;34m(text, encoding, errors)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc4 in position 0: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "np.savetxt(\"cuttext.txt\",df['content_new'].values,fmt=\"%s\")\n",
    "w2v_model = word2vec_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(y,max_features,embedding_dims,filters):\n",
    "\tkernel_size = 3\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(max_features, embedding_dims))        # 使用Embedding层将每个词编码转换为词向量\n",
    "\tmodel.add(Conv1D(filters,\n",
    "\t\t\t\t\t kernel_size,\n",
    "\t\t\t\t\t padding='valid',\n",
    "\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\t strides=1))\n",
    "\t# 池化\n",
    "\tmodel.add(GlobalMaxPooling1D())\n",
    "\n",
    "\tmodel.add(Dense(y.shape[1], activation='softmax')) #第一个参数units: 全连接层输出的维度，即下一层神经元的个数。\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.compile(loss='categorical_crossentropy',\n",
    "\t\t\t\t  optimizer='adam',\n",
    "\t\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "\tmodel.summary()\n",
    "\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\programtools\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From d:\\programtools\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         512000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 200)         77000     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 39)                7839      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 39)                0         \n",
      "=================================================================\n",
      "Total params: 596,839\n",
      "Trainable params: 596,839\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model(train_y_cat,max_word_len,128,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n",
      "WARNING:tensorflow:From d:\\programtools\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From d:\\programtools\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 24882 samples, validate on 6221 samples\n",
      "Epoch 1/10\n",
      " - 10s - loss: 3.9804 - acc: 0.5788 - val_loss: 0.7427 - val_acc: 0.7542\n",
      "Epoch 2/10\n",
      " - 7s - loss: 3.5873 - acc: 0.6557 - val_loss: 0.6332 - val_acc: 0.7885\n",
      "Epoch 3/10\n",
      " - 7s - loss: 3.5510 - acc: 0.6797 - val_loss: 0.6274 - val_acc: 0.7860\n",
      "Epoch 4/10\n",
      " - 7s - loss: 3.4858 - acc: 0.7054 - val_loss: 0.5833 - val_acc: 0.8060\n",
      "Epoch 5/10\n",
      " - 7s - loss: 3.4738 - acc: 0.7223 - val_loss: 0.6129 - val_acc: 0.8061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26e12e00550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " earlyStopping = keras.callbacks.EarlyStopping(\n",
    "\t\t\tmonitor='val_loss',\n",
    "\t\t\tpatience=1,  # 当early stop被激活（如发现loss相比上一个epoch训练没有下降），则经过patience个epoch后停止训练。\n",
    "\t\t\tverbose=0,\n",
    "\t\t\tmode='auto')\n",
    "\n",
    "print(\"training model\")\n",
    "model.fit(train_x,train_y_cat,validation_split=0.2,batch_size=32,epochs=10,verbose=2,callbacks=[earlyStopping],shuffle=True,class_weight='auto') #自动设置class weight让每类的sample对损失的贡献相等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3456/3456 [==============================] - 0s 74us/step\n",
      "test loss:  0.5831887171500258\n",
      "test accuracy:  0.8041087962962963\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_x, test_y_cat)\n",
    "print('test loss: ', loss)\n",
    "print('test accuracy: ', accuracy)\n",
    "#np.savetxt('res_{}_{}.csv'.format(time.strftime(\"%Y%m%d\"),accuracy),res_index)\n",
    "model.save('text_cnn_{}_{}.h5'.format(time.strftime(\"%Y%m%d\"),accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cnn(y,maxlen,max_features,embedding_dims,filters = 250):\n",
    "\t#Inputs\n",
    "\tseq = Input(shape=[maxlen],name='x_seq')\n",
    "\n",
    "\t#Embedding layers\n",
    "\temb = Embedding(max_features,embedding_dims)(seq)\n",
    "\n",
    "\t# conv layers\n",
    "\tconvs = []\n",
    "\tfilter_sizes = [2,3,4,5]\n",
    "\tfor fsz in filter_sizes:\n",
    "\t\tconv1 = Conv1D(filters,kernel_size=fsz,activation='tanh')(emb)\n",
    "\t\tpool1 = MaxPooling1D(maxlen-fsz+1)(conv1)\n",
    "\t\tpool1 = Flatten()(pool1)\n",
    "\t\tconvs.append(pool1)\n",
    "\tmerge = concatenate(convs,axis=1)\n",
    "\n",
    "\tout = Dropout(0.5)(merge)\n",
    "\toutput = Dense(32,activation='relu')(out)\n",
    "\n",
    "\toutput = Dense(units=y.shape[1],activation='sigmoid')(output)\n",
    "\n",
    "\tmodel = Model([seq],output)\n",
    "\tmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_w2v(y,max_features,embedding_dims,filters,maxlen):\n",
    "\t# CNN参数\n",
    "\tkernel_size = 3\n",
    "\n",
    "\tmodel = gensim.models.Word2Vec.load('./predictor/model/word2vec')\n",
    "\tword2idx = {\"_PAD\": 0}  # 初始化 `[word : token]` 字典，后期 tokenize 语料库就是用该词典。\n",
    "\tvocab_list = [(k, model.wv[k]) for k, v in model.wv.vocab.items()]\n",
    "\t# 存储所有 word2vec 中所有向量的数组，留意其中多一位，词向量全为 0， 用于 padding\n",
    "\tembeddings_matrix = np.zeros((len(model.wv.vocab.items()) + 1, model.vector_size))\n",
    "\tprint('Found %s word vectors.' % len(model.wv.vocab.items()))\n",
    "\tfor i in range(len(vocab_list)):\n",
    "\t\tword = vocab_list[i][0]\n",
    "\t\tword2idx[word] = i + 1\n",
    "\t\tembeddings_matrix[i + 1] = vocab_list[i][1]\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\t# 使用Embedding层将每个词编码转换为词向量\n",
    "\tmodel.add(Embedding(len(embeddings_matrix),       #表示文本数据中词汇的取值可能数,从语料库之中保留多少个单词。 因为Keras需要预留一个全零层， 所以+1\n",
    "\t\t\t\t\t\t\t\tembedding_dims,       # 嵌入单词的向量空间的大小。它为每个单词定义了这个层的输出向量的大小\n",
    "\t\t\t\t\t\t\t\tweights=[embeddings_matrix], #构建一个[num_words, EMBEDDING_DIM]的矩阵,然后遍历word_index，将word在W2V模型之中对应vector复制过来。换个方式说：embedding_matrix 是原始W2V的子集，排列顺序按照Tokenizer在fit之后的词顺序。作为权重喂给Embedding Layer\n",
    "\t\t\t\t\t\t\t\tinput_length=maxlen,     # 输入序列的长度，也就是一次输入带有的词汇个数\n",
    "\t\t\t\t\t\t\t\ttrainable=False        # 我们设置 trainable = False，代表词向量不作为参数进行更新\n",
    "\t\t\t\t\t\t))\n",
    "\tmodel.add(Conv1D(filters,\n",
    "\t\t\t\t\t kernel_size,\n",
    "\t\t\t\t\t padding='valid',\n",
    "\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\t strides=1))\n",
    "\t# 池化\n",
    "\tmodel.add(GlobalMaxPooling1D())\n",
    "\n",
    "\tmodel.add(Dense(y.shape[1], activation='softmax')) #第一个参数units: 全连接层输出的维度，即下一层神经元的个数。\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.compile(loss='categorical_crossentropy',\n",
    "\t\t\t\t  optimizer='adam',\n",
    "\t\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "\tmodel.summary()\n",
    "\n",
    "\treturn model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

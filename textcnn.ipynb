{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "E:\\Programfile\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import keras \n",
    "import jieba\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "import gensim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout\n",
    "from keras.layers import Dense,Dropout,Activation,Input,Convolution1D,Conv1D,GlobalAveragePooling1D,GlobalMaxPooling1D,MaxPooling1D,Flatten,concatenate,Embedding,GRU,Lambda, LSTM, TimeDistributed\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Embedding, LSTM,Flatten\n",
    "from keras.models import Model\n",
    "from gensim.models import word2vec\n",
    "import time\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "np.set_printoptions(threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_train = pd.read_csv('age_train.csv',names=['uid','labels'])\n",
    "age_test = pd.read_csv('age_test.csv',names=['uid'])\n",
    "#app_info = pd.read_csv('app_info.csv', names=['appId','category'])\n",
    "user_app_actived = pd. read_csv('user_app_actived.csv',names=['uid','appId'])\n",
    "user_app_actived['appId_list'] = user_app_actived['appId'].apply(lambda x : x.split('#'))\n",
    "age_train = age_train[:1000000]\n",
    "train_data = age_train.merge(user_app_actived,on='uid')\n",
    "test_data = age_test.merge(user_app_actived,on='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "fastmodel = word2vec.Word2Vec(list(user_app_actived['appId_list']), size=embed_size, window=4, min_count=5, negative=2,\n",
    "                     sg=1, sample=0.002, hs=1, workers=4)\n",
    "\n",
    "fastmodel.save('./word2vec.m')\n",
    "print('finished and saved!')\n",
    "\n",
    "embedding_fast = pd.DataFrame([fastmodel[word]\n",
    "                               for word in (fastmodel.wv.vocab)])\n",
    "embedding_fast['app'] = list(fastmodel.wv.vocab)\n",
    "embedding_fast.columns = [\"fdim_%s\" %\n",
    "                          str(i) for i in range(embed_size)]+[\"app\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fastmodel = gensim.models.Word2Vec.load('./word2vec.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=False, char_level=False, split='#')\n",
    "tokenizer.fit_on_texts(list(user_app_actived['appId']))\n",
    "X_seq = tokenizer.texts_to_sequences(train_data['appId'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_data['appId'])\n",
    "train_y = train_data['labels']\n",
    "del train_data\n",
    "del test_data\n",
    "del user_app_actived\n",
    "maxlen = 879\n",
    "X = pad_sequences(X_seq,maxlen= maxlen ,value=0)\n",
    "X_test = pad_sequences(X_test_seq,maxlen=maxlen ,value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "max_feaures = 9401\n",
    "embedding_matrix = np.zeros((max_feaures, embed_size))\n",
    "for word in tokenizer.word_index:\n",
    "    if word not in fastmodel.wv.vocab:\n",
    "        continue\n",
    "    embedding_matrix[tokenizer.word_index[word]] = fastmodel[word]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_age_conv(embedding_matrix):\n",
    "\n",
    "    # The embedding layer containing the word vectors\n",
    "    K.clear_session()\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=embedding_matrix.shape[0],\n",
    "        output_dim=embedding_matrix.shape[1],\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=maxlen,\n",
    "        trainable=False\n",
    "    )\n",
    "    lstm_layer = Bidirectional(\n",
    "        GRU(128, recurrent_dropout=0.15, dropout=0.15, return_sequences=True))\n",
    "\n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1,\n",
    "                   padding='same', activation='relu',)\n",
    "    conv2 = Conv1D(filters=64, kernel_size=2,\n",
    "                   padding='same', activation='relu', )\n",
    "    conv3 = Conv1D(filters=64, kernel_size=3,\n",
    "                   padding='same', activation='relu',)\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5,\n",
    "                   padding='same', activation='relu',)\n",
    "\n",
    "    # Define inputs\n",
    "    seq = Input(shape=(maxlen,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb = emb_layer(seq)\n",
    "\n",
    "    lstm = lstm_layer(emb)\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1a = conv1(lstm)\n",
    "    gap1a = GlobalAveragePooling1D()(conv1a)\n",
    "    gmp1a = GlobalMaxPool1D()(conv1a)\n",
    "\n",
    "    conv2a = conv2(lstm)\n",
    "    gap2a = GlobalAveragePooling1D()(conv2a)\n",
    "    gmp2a = GlobalMaxPool1D()(conv2a)\n",
    "\n",
    "    conv3a = conv3(lstm)\n",
    "    gap3a = GlobalAveragePooling1D()(conv3a)\n",
    "    gmp3a = GlobalMaxPooling1D()(conv3a)\n",
    "\n",
    "    conv5a = conv5(lstm)\n",
    "    gap5a = GlobalAveragePooling1D()(conv5a)\n",
    "    gmp5a = GlobalMaxPooling1D()(conv5a)\n",
    "\n",
    "    merge1 = concatenate([gap1a, gap2a, gap3a, gap5a])\n",
    "\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.3)(merge1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(200, activation='relu',)(x)\n",
    "    x = Dropout(0.22)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(200, activation='relu',)(x)\n",
    "    x = Dropout(0.22)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(200, activation='relu',)(x)\n",
    "    x = Dropout(0.22)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(11, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=seq, outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=AdamW(weight_decay=0.08,))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 879)               0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 879, 128)          1203328   \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 878, 128)          32896     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,244,870\n",
      "Trainable params: 1,244,870\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "seq = Input(shape=[maxlen])\n",
    "# 使用Embedding层将每个词编码转换为词向量\n",
    "#Embedding layers\n",
    "emb = Embedding(len(embedding_matrix),       #表示文本数据中词汇的取值可能数,从语料库之中保留多少个单词。 因为Keras需要预留一个全零层， 所以+1\n",
    "                            embed_size,       # 嵌入单词的向量空间的大小。它为每个单词定义了这个层的输出向量的大小\n",
    "                            weights=[embedding_matrix], #构建一个[num_words, EMBEDDING_DIM]的矩阵,然后遍历word_index，将word在W2V模型之中对应vector复制过来。换个方式说：embedding_matrix 是原始W2V的子集，排列顺序按照Tokenizer在fit之后的词顺序。作为权重喂给Embedding Layer\n",
    "                            input_length=maxlen,     # 输入序列的长度，也就是一次输入带有的词汇个数\n",
    "                            trainable=True        # 我们设置 trainable = False，代表词向量不作为参数进行更新\n",
    "                    )(seq)\n",
    "\n",
    "convs = []\n",
    "filter_sizes = [2,]\n",
    "filter_nums = [128,]\n",
    "for fsz,fnums in zip(filter_sizes,filter_nums):\n",
    "    conv1 = Conv1D(filters=fnums,kernel_size=fsz,activation='relu')(emb)\n",
    "    pool1 = MaxPooling1D(int(conv1.shape[1]))(conv1)\n",
    "    pool1 = Flatten()(pool1)\n",
    "    convs.append(pool1)\n",
    "#merge = concatenate(convs,axis=1)\n",
    "#out = Dropout(0.2)(merge)\n",
    "out = Dropout(0.2)(convs[0])\n",
    "\n",
    "output = Dense(64,activation='relu')(out)\n",
    "\n",
    "output = Dense(units=6,activation='sigmoid')(output)\n",
    "\n",
    "model = Model([seq],output)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='RMSprop',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_cat = to_categorical(train_y-1,num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n",
      "Train on 980000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "918016/980000 [===========================>..] - ETA: 1:09 - loss: 1.5275 - acc: 0.2192"
     ]
    }
   ],
   "source": [
    "earlyStopping = keras.callbacks.EarlyStopping(\n",
    "\t\t\tmonitor='val_loss',\n",
    "\t\t\tpatience=0,  # 当early stop被激活（如发现loss相比上一个epoch训练没有下降），则经过patience个epoch后停止训练。\n",
    "\t\t\tverbose=0,\n",
    "\t\t\tmode='auto')\n",
    "\n",
    "print(\"training model\")\n",
    "model.fit(X,train_y_cat,validation_split=0.02,batch_size=32,epochs=10,verbose=1,callbacks=[earlyStopping],shuffle=True,class_weight='auto') #自动设置class weight让每类的sample对损失的贡献相等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[0][-19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.loc[0]['appId_list'][-19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[169568][850]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index['a00184278']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_w2v(y_cat_nums,max_features,embedding_dims,filters,maxlen):\n",
    "\t# CNN参数\n",
    "\tkernel_size = 3\n",
    "\n",
    "\tw2v_model = gensim.models.Word2Vec.load('./word2vec.m')\n",
    "\tword2idx = {\"_PAD\": 0}  # 初始化 `[word : token]` 字典，后期 tokenize 语料库就是用该词典。\n",
    "\tvocab_list = [(k, w2v_model.wv[k]) for k, v in w2v_model.wv.vocab.items()]\n",
    "\t# 存储所有 word2vec 中所有向量的数组，留意其中多一位，词向量全为 0， 用于 padding\n",
    "\tembeddings_matrix = np.zeros((len(w2v_model.wv.vocab.items()) + 1, w2v_model.vector_size))\n",
    "\tprint('Found %s word vectors.' % len(w2v_model.wv.vocab.items()))\n",
    "\tfor i in range(len(vocab_list)):\n",
    "\t\tword = vocab_list[i][0]\n",
    "\t\tword2idx[word] = i + 1\n",
    "\t\tembeddings_matrix[i + 1] = vocab_list[i][1]\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\t# 使用Embedding层将每个词编码转换为词向量\n",
    "\tmodel.add(Embedding(len(embeddings_matrix),       #表示文本数据中词汇的取值可能数,从语料库之中保留多少个单词。 因为Keras需要预留一个全零层， 所以+1\n",
    "\t\t\t\t\t\t\t\tembedding_dims,       # 嵌入单词的向量空间的大小。它为每个单词定义了这个层的输出向量的大小\n",
    "\t\t\t\t\t\t\t\tweights=[embeddings_matrix], #构建一个[num_words, EMBEDDING_DIM]的矩阵,然后遍历word_index，将word在W2V模型之中对应vector复制过来。换个方式说：embedding_matrix 是原始W2V的子集，排列顺序按照Tokenizer在fit之后的词顺序。作为权重喂给Embedding Layer\n",
    "\t\t\t\t\t\t\t\tinput_length=maxlen,     # 输入序列的长度，也就是一次输入带有的词汇个数\n",
    "\t\t\t\t\t\t\t\ttrainable=True        # 我们设置 trainable = False，代表词向量不作为参数进行更新\n",
    "\t\t\t\t\t\t))\n",
    "\tmodel.add(Conv1D(filters,\n",
    "\t\t\t\t\t kernel_size,\n",
    "\t\t\t\t\t padding='valid',\n",
    "\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\t strides=1))\n",
    "\t# 池化\n",
    "\tmodel.add(GlobalMaxPooling1D())\n",
    "\n",
    "\tmodel.add(Dense(y_cat_nums, activation='softmax')) #第一个参数units: 全连接层输出的维度，即下一层神经元的个数。\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.compile(loss='categorical_crossentropy',\n",
    "\t\t\t\t  optimizer='adam',\n",
    "\t\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "\tmodel.summary()\n",
    "\n",
    "\treturn model,word2idx\n",
    "cnn_model,word2idx = cnn_w2v(6,100,100,200,879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec.load('./word2vec.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list = [(k, w2v_model.wv[k]) for k, v in w2v_model.wv.vocab.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_cat = to_categorical(total_data[:1000000]['labels']-1,num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earlyStopping = keras.callbacks.EarlyStopping(\n",
    "\t\t\tmonitor='val_loss',\n",
    "\t\t\tpatience=1,  # 当early stop被激活（如发现loss相比上一个epoch训练没有下降），则经过patience个epoch后停止训练。\n",
    "\t\t\tverbose=0,\n",
    "\t\t\tmode='auto')\n",
    "\n",
    "print(\"training model\")\n",
    "cnn_model.fit(X[:1000000],train_y_cat,validation_split=0.2,batch_size=32,epochs=10,verbose=1,callbacks=[earlyStopping],shuffle=True,class_weight='auto') #自动设置class weight让每类的sample对损失的贡献相等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_cnn(y,maxlen,max_features,embedding_dims,filters = 250):\n",
    "\t#Inputs\n",
    "\tseq = Input(shape=[maxlen],name='x_seq')\n",
    "\n",
    "\t#Embedding layers\n",
    "\temb = Embedding(max_features,embedding_dims)(seq)\n",
    "\n",
    "\t# conv layers\n",
    "\tconvs = []\n",
    "\tfilter_sizes = [2,3,4,5]\n",
    "\tfor fsz in filter_sizes:\n",
    "\t\tconv1 = Conv1D(filters,kernel_size=fsz,activation='tanh')(emb)\n",
    "\t\tpool1 = MaxPooling1D(maxlen-fsz+1)(conv1)\n",
    "\t\tpool1 = Flatten()(pool1)\n",
    "\t\tconvs.append(pool1)\n",
    "\tmerge = concatenate(convs,axis=1)\n",
    "\n",
    "\tout = Dropout(0.2)(merge)\n",
    "\toutput = Dense(32,activation='relu')(out)\n",
    "\n",
    "\toutput = Dense(units=y.shape[1],activation='sigmoid')(output)\n",
    "\n",
    "\tmodel = Model([seq],output)\n",
    "\tmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
